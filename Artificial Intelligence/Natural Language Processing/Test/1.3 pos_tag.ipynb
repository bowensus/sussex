{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Question 2\n",
    "\n",
    "This Question is about POS Tagging.\n",
    "\n"
   ],
   "metadata": {
    "id": "grPE5uwpVI65"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7KspiAfrVF_n",
    "ExecuteTime": {
     "end_time": "2024-01-14T21:00:28.534771800Z",
     "start_time": "2024-01-14T21:00:28.526860900Z"
    }
   },
   "outputs": [],
   "source": [
    "### do not change the code in this cell\n",
    "# make sure you run this cell\n",
    "tagged_sentences=[\"john_N loves_V chocolate_N\",\n",
    "                  \"bob_N hates_V meetings_N\",\n",
    "                  \"alice_N likes_V fred_N\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "a) \n",
    "\n",
    "i) Use the `split` method to break each sentence in `tagged_sentences` into `word_tag` tokens. The result should be a list of lists and be named `tagged_words` For example, `[\"dogs_N bark_V\"]` should become `[[\"dogs_N\",\"bark_V\"]]`. (4 marks)\n",
    "\n"
   ],
   "metadata": {
    "id": "LgT47zd8WJNm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tagged_words = [sentence.split() for sentence in tagged_sentences]\n",
    "print(tagged_words)\n"
   ],
   "metadata": {
    "id": "CkoECVuwdxrJ",
    "ExecuteTime": {
     "end_time": "2024-01-14T21:00:30.816287600Z",
     "start_time": "2024-01-14T21:00:30.813781200Z"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['john_N', 'loves_V', 'chocolate_N'], ['bob_N', 'hates_V', 'meetings_N'], ['alice_N', 'likes_V', 'fred_N']]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "ii) Use the `split` method again to break each token in `tagged_words` into `(word,tag)` tuples. The result should be a list of lists of tuples and be named `words_and_tags` For example, `[[\"dogs_N\", \"bark_V\"]]` should become `[[(\"dogs\",\"N\"), (\"bark\",\"V\")]]`. (6 marks)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "_0cSNormdyz-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def pos_tokens(tokens):\n",
    "    pos_words = []\n",
    "    for i, sent in enumerate(tokens):\n",
    "        pos_words.append([])\n",
    "        for token in sent:\n",
    "            word, pos = token.split('_')\n",
    "            pos_words[i].append((word, pos))\n",
    "    return pos_words\n",
    "\n",
    "words_and_tags = pos_tokens(tagged_words)\n",
    "print(words_and_tags)"
   ],
   "metadata": {
    "id": "36U7HvzfeyB5",
    "ExecuteTime": {
     "end_time": "2024-01-14T21:00:33.124262100Z",
     "start_time": "2024-01-14T21:00:33.102315500Z"
    }
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('john', 'N'), ('loves', 'V'), ('chocolate', 'N')], [('bob', 'N'), ('hates', 'V'), ('meetings', 'N')], [('alice', 'N'), ('likes', 'V'), ('fred', 'N')]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "iii) Calculate the counts for all tags in `words_and_tags`. The result should be a dictionary called `tag_counts`. For example, the tag \"V\" occurs 3 times, so `tag_counts[\"V\"]` will be `3`. (4 marks)"
   ],
   "metadata": {
    "id": "RIuwCLsPeyvv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_tag_counts(tags):\n",
    "    tag_counts = {}\n",
    "    for sent in tags:\n",
    "        for tag in sent:\n",
    "            tag_counts[tag[1]] = tag_counts.get(tag[1], 0) + 1\n",
    "    return tag_counts\n",
    "\n",
    "tag_counts = get_tag_counts(words_and_tags)\n",
    "print(tag_counts)"
   ],
   "metadata": {
    "id": "f62Qza4XfbGj",
    "ExecuteTime": {
     "end_time": "2024-01-14T21:02:02.139367500Z",
     "start_time": "2024-01-14T21:02:02.137350100Z"
    }
   },
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'N': 6, 'V': 3}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "iv) Calculate the tag counts for all words in `words_and_tags` and call the result `word_tag_counts`. This should be a dictionary of dictionaries with the outer dictionary having words as keys and the values being themselves dictionaries that contain the tag counts for that word. For example, `word_tag_counts[\"john\"][\"N\"]=1`. (6 marks)"
   ],
   "metadata": {
    "id": "RdPWLXiNfbjA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def calculate_word_tag_counts(tags):\n",
    "    word_tag_counts = {}\n",
    "    for sent in tags:\n",
    "        for tag in sent:\n",
    "            word_tag_counts.update({tag[0]:{pos:0 for pos in tag_counts.keys()}})\n",
    "            word_tag_counts[tag[0]][tag[1]] += 1\n",
    "    return word_tag_counts\n",
    "\n",
    "word_tag_counts = calculate_word_tag_counts(words_and_tags)\n",
    "print(word_tag_counts)"
   ],
   "metadata": {
    "id": "xQU64ejVg5_0",
    "ExecuteTime": {
     "end_time": "2024-01-14T21:12:35.279893500Z",
     "start_time": "2024-01-14T21:12:35.269594900Z"
    }
   },
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'john': {'N': 1, 'V': 0}, 'loves': {'N': 0, 'V': 1}, 'chocolate': {'N': 1, 'V': 0}, 'bob': {'N': 1, 'V': 0}, 'hates': {'N': 0, 'V': 1}, 'meetings': {'N': 1, 'V': 0}, 'alice': {'N': 1, 'V': 0}, 'likes': {'N': 0, 'V': 1}, 'fred': {'N': 1, 'V': 0}}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "b) Name three other common part-of-speech classes, and an example of a word that is found in each class. (6 marks)"
   ],
   "metadata": {
    "id": "ILO3VZ6RhHZY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Proper Noun (NNP):\n",
    "Examples: \"London,\" \"Alice,\" \"Microsoft\"\n",
    "\n",
    "Adverb (RB):\n",
    "Examples: \"quickly,\" \"loudly,\" \"never\"\n",
    "\n",
    "Pronoun (PRP):\n",
    "Examples: \"he,\" \"she,\" \"it\""
   ],
   "metadata": {
    "id": "9m5dAMi1EMZl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "c) Construct a unigram tagger by following the steps below."
   ],
   "metadata": {
    "id": "Rrr2XXy5EM-i"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "i) Calculate the tag probabilities p(t) from the `tag_counts` dictionary by dividing each tag count by the total count and put the result in a dictionary called `tag_probs`. (3 marks)"
   ],
   "metadata": {
    "id": "bIJklFHwEjNA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "total_count = sum(tag_counts.values())\n",
    "tag_probs = {tag: count / total_count for tag, count in tag_counts.items()}\n",
    "\n",
    "print(tag_probs)\n"
   ],
   "metadata": {
    "id": "4GfOLhidFC41",
    "ExecuteTime": {
     "end_time": "2024-01-14T21:15:48.731944800Z",
     "start_time": "2024-01-14T21:15:48.729941900Z"
    }
   },
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'N': 0.6666666666666666, 'V': 0.3333333333333333}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "ii) Calculate the emission probabilities p(w|t) from the `word_tag_counts` dictionary by dividing each word-tag count by the total count for each tag, and put the result in a dictionary called `word_tag_probs`. (5 marks)"
   ],
   "metadata": {
    "id": "UoB_NrryFDcA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Calculate the emission probabilities p(w|t)\n",
    "word_tag_probs = {}\n",
    "for word, tag_counts in word_tag_counts.items():\n",
    "    word_tag_probs[word] = {tag: count / total_count for tag, count in word_tag_counts[word].items()}\n",
    "\n",
    "print(word_tag_probs)\n"
   ],
   "metadata": {
    "id": "PZM4k95cFhgJ",
    "ExecuteTime": {
     "end_time": "2024-01-14T21:21:19.844070700Z",
     "start_time": "2024-01-14T21:21:19.841563900Z"
    }
   },
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'john': {'N': 0.1111111111111111, 'V': 0.0}, 'loves': {'N': 0.0, 'V': 0.1111111111111111}, 'chocolate': {'N': 0.1111111111111111, 'V': 0.0}, 'bob': {'N': 0.1111111111111111, 'V': 0.0}, 'hates': {'N': 0.0, 'V': 0.1111111111111111}, 'meetings': {'N': 0.1111111111111111, 'V': 0.0}, 'alice': {'N': 0.1111111111111111, 'V': 0.0}, 'likes': {'N': 0.0, 'V': 0.1111111111111111}, 'fred': {'N': 0.1111111111111111, 'V': 0.0}}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "iii) Write a function that takes a list of words and outputs the most probable tag for each word. Apply your function to the list `[\"fred\",\"likes\",\"meetings\"]`. (10 marks)"
   ],
   "metadata": {
    "id": "J9h_nl-pIqeV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def most_probable_tags(words, tag_probs, word_tag_probs):\n",
    "    result_tags = []\n",
    "    for w in words:\n",
    "        if w in word_tag_probs:\n",
    "            most_probable_tag = max(word_tag_probs[w], key=word_tag_probs[w].get)\n",
    "        else:\n",
    "            most_probable_tag = max(tag_probs, key=tag_probs.get)\n",
    "        result_tags.append(most_probable_tag)\n",
    "    return result_tags\n",
    "\n",
    "input_words = [\"fred\", \"likes\", \"meetings\"]\n",
    "result_tags = most_probable_tags(input_words, tag_probs, word_tag_probs)\n",
    "\n",
    "print(result_tags)\n"
   ],
   "metadata": {
    "id": "OQLnGU-ON5iP",
    "ExecuteTime": {
     "end_time": "2024-01-14T21:24:37.306042Z",
     "start_time": "2024-01-14T21:24:37.303228800Z"
    }
   },
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N', 'V', 'N']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "d)\n",
    "\n",
    "i) The unigram tagger defined above ignores the order of tags and words. A better approach is to assign probabilities to sequences of tags, rather than to each tag individually. Describe the Markov assumption as it applies in this case. (2 marks)"
   ],
   "metadata": {
    "id": "DY7JDPE-N-at"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "yBLgfoH7O3AH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ii) In the case where we are trying to find the most probable sequence of tags, the simplest approach would be to calculate a probability for every possible assignment of tags to words. Why might we want to avoid doing this, and what algorithm can we use instead? (4 marks)"
   ],
   "metadata": {
    "id": "rfc8ne91O39T"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The special calculation method is a kind of dynamic calculation method, which can be used easily, which can be used more effectively, which makes it easier to calculate the most possible order, and to avoid exhaustive search."
   ],
   "metadata": {
    "id": "K6VFkX4hS9ET"
   }
  }
 ]
}
