{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#Question 2\n",
    "\n",
    "This questions is about document classification."
   ],
   "metadata": {
    "id": "UTDVYEeyZxPS"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tftq795GZuDb",
    "ExecuteTime": {
     "end_time": "2024-01-14T18:18:08.689485500Z",
     "start_time": "2024-01-14T18:18:08.381218800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### do not change the code in this cell\n",
    "# make sure you run this cell\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "positive_reviews=[\"great product\",\n",
    "                  \"excellent value\",\n",
    "                  \"performance was excellent\",\n",
    "                  \"very very good\",\n",
    "                  \"best of its kind\"]\n",
    "\n",
    "negative_reviews=[\"extremely poor\",\n",
    "                  \"would not recommend\",\n",
    "                  \"did not work\",\n",
    "                  \"very poor product\",\n",
    "                  \"not ideal\"]\n",
    "\n",
    "unlabelled_review=\"great value\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "a) Why is stopword removal often applied in document classification tasks? What impact might it have on a classifier trained on the reviews above? (4 marks)"
   ],
   "metadata": {
    "id": "B2QfbNcvV-cK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Removing stop words can improve text processing efficiency and increase the attention of the text, which are usually \n",
    "important words that pay more attention to the emotion of the text."
   ],
   "metadata": {
    "id": "sr8zty9dWTIU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "b)\n",
    "\n",
    "i) Tokenize the reviews in the lists `positive_reviews` and `negative_reviews` to produce two new lists called `positive_tokenized` and `negative_tokenized`. For example, the list `[\"this is a review\", \"this is another\"]` would become `[[\"this\", \"is\", \"a\", \"review\"], [\"this\", \"is\", \"another\"]]`. (6 marks)"
   ],
   "metadata": {
    "id": "Cfslnjtbb12B"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_tokens(reviews):\n",
    "    tokens = []\n",
    "    for phrase in reviews:\n",
    "        tokens.append(word_tokenize(phrase))\n",
    "    return tokens    \n",
    "    \n",
    "positive_tokenized = get_tokens(positive_reviews)\n",
    "negative_tokenized = get_tokens(negative_reviews)\n",
    "\n",
    "print(positive_tokenized)\n",
    "print(negative_tokenized)"
   ],
   "metadata": {
    "id": "t87u3PS2cNMh",
    "ExecuteTime": {
     "end_time": "2024-01-14T18:18:10.445656700Z",
     "start_time": "2024-01-14T18:18:10.436273300Z"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['great', 'product'], ['excellent', 'value'], ['performance', 'was', 'excellent'], ['very', 'very', 'good'], ['best', 'of', 'its', 'kind']]\n",
      "[['extremely', 'poor'], ['would', 'not', 'recommend'], ['did', 'not', 'work'], ['very', 'poor', 'product'], ['not', 'ideal']]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "ii) Construct a training dataset from the lists of positive and negative tokenized reviews, consisting of a list of two element tuples in which the first element contains the list of tokens and the second element is either \"pos\" or \"neg\", indicating the sentiment of the review. For example, the positive review `[\"a\", \"good\", \"review\"]` would be represented in the training set as `([\"a\", \"good\", \"review\"], \"pos\")` (4 marks)"
   ],
   "metadata": {
    "id": "GroEkOZgcMYx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def tag_tokens(tokens, category):\n",
    "    tokens_tag = []\n",
    "    for token in tokens:\n",
    "        if category == \"pos\":\n",
    "            tokens_tag.append((token, \"pos\"))\n",
    "        elif category == \"neg\":\n",
    "            tokens_tag.append((token, \"neg\"))\n",
    "    return tokens_tag\n",
    "\n",
    "positive_tokenized_tag = tag_tokens(positive_tokenized, \"pos\")\n",
    "negative_tokenized_tag = tag_tokens(negative_tokenized, \"neg\")\n",
    "\n",
    "print(positive_tokenized_tag)\n",
    "print(negative_tokenized_tag)"
   ],
   "metadata": {
    "id": "YtqPrbVTeL0q",
    "ExecuteTime": {
     "end_time": "2024-01-14T18:25:11.948334200Z",
     "start_time": "2024-01-14T18:25:11.924762900Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['great', 'product'], 'pos'), (['excellent', 'value'], 'pos'), (['performance', 'was', 'excellent'], 'pos'), (['very', 'very', 'good'], 'pos'), (['best', 'of', 'its', 'kind'], 'pos')]\n",
      "[(['extremely', 'poor'], 'neg'), (['would', 'not', 'recommend'], 'neg'), (['did', 'not', 'work'], 'neg'), (['very', 'poor', 'product'], 'neg'), (['not', 'ideal'], 'neg')]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "iii) Convert the reviews to a bag-of-words representation using the FreqDist class. For example, the document `[\"this\", \"is\", \"a\", \"review\"]` would become `FreqDist({'this': 1, 'is': 1, 'a': 1, 'review': 1})`. The result should a be a list of pairs, in which one item is the bag-of-words review representation and the other is the sentiment label. (6 marks)"
   ],
   "metadata": {
    "id": "nqyB8-sqejm6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def bag_of_words(tokens_tag):\n",
    "    bag = []\n",
    "    for token in tokens_tag:\n",
    "        bag.append((FreqDist(token[0]), token[1]))\n",
    "    return bag\n",
    "\n",
    "pos_bag = bag_of_words(positive_tokenized_tag)\n",
    "neg_bag = bag_of_words(negative_tokenized_tag)\n",
    "\n",
    "print(pos_bag)\n",
    "print(neg_bag)"
   ],
   "metadata": {
    "id": "-9Kl_ItueoaU",
    "ExecuteTime": {
     "end_time": "2024-01-14T18:36:53.908222300Z",
     "start_time": "2024-01-14T18:36:53.902236700Z"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(FreqDist({'great': 1, 'product': 1}), 'pos'), (FreqDist({'excellent': 1, 'value': 1}), 'pos'), (FreqDist({'performance': 1, 'was': 1, 'excellent': 1}), 'pos'), (FreqDist({'very': 2, 'good': 1}), 'pos'), (FreqDist({'best': 1, 'of': 1, 'its': 1, 'kind': 1}), 'pos')]\n",
      "[(FreqDist({'extremely': 1, 'poor': 1}), 'neg'), (FreqDist({'would': 1, 'not': 1, 'recommend': 1}), 'neg'), (FreqDist({'did': 1, 'not': 1, 'work': 1}), 'neg'), (FreqDist({'very': 1, 'poor': 1, 'product': 1}), 'neg'), (FreqDist({'not': 1, 'ideal': 1}), 'neg')]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "c)\n",
    "\n",
    "i) From the training data, calculate the prior probabilities, $p(c)$, of a review being in each sentiment class, $c \\in \\{ pos , neg \\}$. \n",
    "$$ p(c) = \\frac{freq(c)}{N}$$\n",
    "Here, $freq(c)$ is the number of documents with label $c$, and $N$ is the total number of documents. The result should be a dictionary whose keys are class labels and whose values are class probabilities. (6 marks)"
   ],
   "metadata": {
    "id": "kKKuhOEDfTP3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pos_num = len(positive_reviews)\n",
    "neg_num = len(negative_reviews)\n",
    "total_num = pos_num + neg_num\n",
    "\n",
    "prior_prob_pos = pos_num / total_num\n",
    "prior_prob_neg = neg_num / total_num\n",
    "\n",
    "prior_probabilities = {\"pos\": prior_prob_pos, \"neg\": prior_prob_neg}\n",
    "\n",
    "print(\"Prior Probabilities:\")\n",
    "print(prior_probabilities)"
   ],
   "metadata": {
    "id": "YUkF5QwKgtEL",
    "ExecuteTime": {
     "end_time": "2024-01-14T18:46:04.775187600Z",
     "start_time": "2024-01-14T18:46:04.773185300Z"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior Probabilities:\n",
      "{'pos': 0.5, 'neg': 0.5}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "ii) Calculate the conditional probabilities, $p(f|c)$, of each token feature, $f$, given each sentiment class, $c$.\n",
    "$$ p(f|c) = \\frac{freq(f,c)}{freq(c)}$$ \n",
    "\n",
    "Where $freq(f,c)$ is the number of documents in class $c$ containing feature $f$ and $freq(c)$ is the total number of documents in class $c$. The result should be a dictionary of dictionaries, with the outer keys being token features, and the inner keys being class labels whose corresponding values are the conditional probabilities. (10 marks)"
   ],
   "metadata": {
    "id": "NYr71EFKhyba"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def freq_words(tokens):\n",
    "    freq = {}\n",
    "    for token in tokens:\n",
    "        for w in set(token):\n",
    "            freq[w] = freq.get(w, 0) + 1\n",
    "    return freq\n",
    "\n",
    "freq_pos_words = freq_words(positive_tokenized)\n",
    "freq_neg_words = freq_words(negative_tokenized)\n",
    "\n",
    "all_unique_words = set(freq_pos_words.keys()).union(freq_neg_words.keys())\n",
    "\n",
    "print(freq_pos_words)\n",
    "\n",
    "def calculate_conditional_probabilities():\n",
    "    conditional_probabilities = {}\n",
    "    for w in all_unique_words:\n",
    "        freq_word_given_pos = freq_pos_words.get(w, 0)\n",
    "        freq_word_given_neg = freq_neg_words.get(w, 0)\n",
    "        \n",
    "        prob_word_given_pos = freq_word_given_pos / len(positive_tokenized)\n",
    "        prob_word_given_neg = freq_word_given_neg / len(negative_tokenized)\n",
    "    \n",
    "        word_probabilities = {\"pos\": prob_word_given_pos, \"neg\": prob_word_given_neg}\n",
    "    \n",
    "        conditional_probabilities[w] = word_probabilities\n",
    "    return conditional_probabilities\n",
    "\n",
    "conditional_probabilities = calculate_conditional_probabilities()    \n",
    "print(\"Conditional Probabilities:\")\n",
    "print(conditional_probabilities)\n"
   ],
   "metadata": {
    "id": "9xyKOBNaicjp",
    "ExecuteTime": {
     "end_time": "2024-01-14T19:20:01.036366100Z",
     "start_time": "2024-01-14T19:20:01.031354900Z"
    }
   },
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'great': 1, 'product': 1, 'excellent': 2, 'value': 1, 'performance': 1, 'was': 1, 'very': 1, 'good': 1, 'best': 1, 'kind': 1, 'its': 1, 'of': 1}\n",
      "Conditional Probabilities:\n",
      "{'would': {'pos': 0.0, 'neg': 0.2}, 'ideal': {'pos': 0.0, 'neg': 0.2}, 'best': {'pos': 0.2, 'neg': 0.0}, 'not': {'pos': 0.0, 'neg': 0.6}, 'great': {'pos': 0.2, 'neg': 0.0}, 'kind': {'pos': 0.2, 'neg': 0.0}, 'value': {'pos': 0.2, 'neg': 0.0}, 'very': {'pos': 0.2, 'neg': 0.2}, 'good': {'pos': 0.2, 'neg': 0.0}, 'poor': {'pos': 0.0, 'neg': 0.4}, 'its': {'pos': 0.2, 'neg': 0.0}, 'of': {'pos': 0.2, 'neg': 0.0}, 'was': {'pos': 0.2, 'neg': 0.0}, 'did': {'pos': 0.0, 'neg': 0.2}, 'recommend': {'pos': 0.0, 'neg': 0.2}, 'performance': {'pos': 0.2, 'neg': 0.0}, 'product': {'pos': 0.2, 'neg': 0.2}, 'extremely': {'pos': 0.0, 'neg': 0.2}, 'work': {'pos': 0.0, 'neg': 0.2}, 'excellent': {'pos': 0.4, 'neg': 0.0}}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "iii) Use the priors and conditional probabilities to make a Naive Bayes prediction of the sentiment class for the unlabelled review. (6 marks)"
   ],
   "metadata": {
    "id": "JgU4yI5YqJka"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def naive_bayes_predict(review, prior_probabilities, conditional_probabilities):\n",
    "    tokenized_review = word_tokenize(review)\n",
    "\n",
    "    prob_pos = prior_probabilities[\"pos\"]\n",
    "    prob_neg = prior_probabilities[\"neg\"]\n",
    "    \n",
    "    smoothing_factor = 0.0\n",
    "    \n",
    "    for w in tokenized_review:\n",
    "        if w in conditional_probabilities:\n",
    "            prob_pos *= (conditional_probabilities[w][\"pos\"] + smoothing_factor)\n",
    "            prob_neg *= (conditional_probabilities[w][\"neg\"] + smoothing_factor)\n",
    "\n",
    "    if prob_pos > prob_neg:\n",
    "        return \"pos\"\n",
    "    else:\n",
    "        return \"neg\"\n",
    "\n",
    "predicted_sentiment = naive_bayes_predict(unlabelled_review, prior_probabilities, conditional_probabilities)\n",
    "\n",
    "print(\"Predicted Sentiment:\", predicted_sentiment)"
   ],
   "metadata": {
    "id": "Q4CI0NYrrBA5",
    "ExecuteTime": {
     "end_time": "2024-01-14T19:31:31.731878400Z",
     "start_time": "2024-01-14T19:31:31.725634900Z"
    }
   },
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: pos\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "d)\n",
    "\n",
    "i) If you apply the Naive Bayes model to the training data using the class priors and conditional probabilities calculated so far, without smoothing, then you will find that you get predictions of 100% accuracy. What can we say about precision, recall and F1 in this case? (2 marks)"
   ],
   "metadata": {
    "id": "6fSLG0dF2Cz0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Precision: Precision is the ratio of true positive predictions to the total positive predictions. Since there are no false positives (all predictions are correct), precision would be 100%.\n",
    "\n",
    "Recall: Recall is the ratio of true positive predictions to the total actual positives. Again, since there are no false negatives (all positive instances are correctly identified), recall would be 100%.\n",
    "\n",
    "F1 Score: The F1 score is the harmonic mean of precision and recall. Since both precision and recall are 100%, the F1 score would also be 100%."
   ],
   "metadata": {
    "id": "lDH-apUw2jyQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ii) Should we apply smoothing in this case? Justify your answer. (6 marks)"
   ],
   "metadata": {
    "id": "wODZIWbS2pk_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reasons to Apply Smoothing:\n",
    "Avoiding Zero Probabilities\n",
    "\n",
    "Reasons Not to Apply Smoothing:\n",
    "In the given scenario where the Naive Bayes model achieves 100% accuracy on the training data, it might be tempting to argue against smoothing since the model appears to perform well without it. \n"
   ],
   "metadata": {
    "id": "VJNS6sOUBlaF"
   }
  }
 ]
}
