{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2S8I2ny-ovS"
   },
   "source": [
    "# ANLP Assignment: Sentiment Classification\n",
    "\n",
    "In this assignment, you will be investigating NLP methods for distinguishing positive and negative reviews written about movies.\n",
    "\n",
    "For assessment, you are expected to complete and submit this notebook file.  When answers require code, you may import and use library functions (unless explicitly told otherwise).  All of your own code should be included in the notebook rather than imported from elsewhere.  Written answers should also be included in the notebook.  You should insert as many extra cells as you want and change the type between code and markdown as appropriate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "1gXQAZas-l9c"
   },
   "outputs": [],
   "source": [
    "candidateno=276318 #this MUST be updated to your candidate number so that you get a unique data sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "nk8JTP88A8vs",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "28f64d4c-0d10-4b02-99d3-53a8f249f52b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#do not change the code in this cell\n",
    "#preliminary imports\n",
    "\n",
    "#set up nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('movie_reviews')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "#for setting up training and testing data\n",
    "import random\n",
    "\n",
    "#useful other tools\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from itertools import zip_longest\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.classify.api import ClassifierI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "BHBkzAccCVaZ"
   },
   "outputs": [],
   "source": [
    "#do not change the code in this cell\n",
    "def split_data(data, ratio=0.7): # when the second argument is not given, it defaults to 0.7\n",
    "    \"\"\"\n",
    "    Given corpus generator and ratio:\n",
    "     - partitions the corpus into training data and test data, where the proportion in train is ratio,\n",
    "\n",
    "    :param data: A corpus generator.\n",
    "    :param ratio: The proportion of training documents (default 0.7)\n",
    "    :return: a pair (tuple) of lists where the first element of the\n",
    "            pair is a list of the training data and the second is a list of the test data.\n",
    "    \"\"\"\n",
    "\n",
    "    data = list(data)\n",
    "    n = len(data)\n",
    "    train_indices = random.sample(range(n), int(n * ratio))\n",
    "    test_indices = list(set(range(n)) - set(train_indices))\n",
    "    train = [data[i] for i in train_indices]\n",
    "    test = [data[i] for i in test_indices]\n",
    "    return (train, test)\n",
    "\n",
    "\n",
    "def get_train_test_data():\n",
    "\n",
    "    #get ids of positive and negative movie reviews\n",
    "    pos_review_ids=movie_reviews.fileids('pos')\n",
    "    neg_review_ids=movie_reviews.fileids('neg')\n",
    "\n",
    "    #split positive and negative data into training and testing sets\n",
    "    pos_train_ids, pos_test_ids = split_data(pos_review_ids)\n",
    "    neg_train_ids, neg_test_ids = split_data(neg_review_ids)\n",
    "    #add labels to the data and concatenate\n",
    "    training = [(movie_reviews.words(f),'pos') for f in pos_train_ids]+[(movie_reviews.words(f),'neg') for f in neg_train_ids]\n",
    "    testing = [(movie_reviews.words(f),'pos') for f in pos_test_ids]+[(movie_reviews.words(f),'neg') for f in neg_test_ids]\n",
    "\n",
    "    return training, testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1N3LWwBYICPP"
   },
   "source": [
    "When you have run the cell below, your unique training and testing samples will be stored in `training_data` and `testing_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "HJLegkdPFUJA",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "272f2164-d98d-43e7-ff5f-b42fb597e212"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The amount of training data is 1400\n",
      "The amount of testing data is 600\n",
      "The representation of a single data item is below\n",
      "(['if', 'there', \"'\", 's', 'one', 'thing', 'in', ...], 'pos')\n"
     ]
    }
   ],
   "source": [
    "#do not change the code in this cell\n",
    "random.seed(candidateno)\n",
    "training_data,testing_data=get_train_test_data()\n",
    "print(\"The amount of training data is {}\".format(len(training_data)))\n",
    "print(\"The amount of testing data is {}\".format(len(testing_data)))\n",
    "print(\"The representation of a single data item is below\")\n",
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbTq6eGv2XT2"
   },
   "source": [
    "1)  \n",
    "a) **Generate** a list of 10 content words which are representative of the positive reviews in your training data.\n",
    "\n",
    "b) **Generate** a list of 10 content words which are representative of the negative reviews in your training data.\n",
    "\n",
    "c) **Explain** what you have done and why\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "For calculating the precision and recall rate, I will distrubute the data into four parts as below. I have build up the positive and negative data, so I remove the tag in the last for less mistakes in code later."
   ],
   "metadata": {
    "id": "_Jo6Lf8Ynuv3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "JXHrtNCg2XT4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "018081c0-5661-481a-e6db-7666652de13e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "700\n",
      "700\n",
      "['if', 'there', \"'\", 's', 'one', 'thing', 'in', ...]\n"
     ]
    }
   ],
   "source": [
    "pos_training_data = [file[0] for file in training_data if file[1] == \"pos\"]\n",
    "neg_training_data = [file[0] for file in training_data if file[1] == \"neg\"]\n",
    "pos_testing_data = [file[0] for file in testing_data if file[1] == \"pos\"]\n",
    "neg_testing_data = [file[0] for file in testing_data if file[1] == \"neg\"]\n",
    "\n",
    "print(len(pos_training_data))\n",
    "print(len(neg_training_data))\n",
    "print(pos_training_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This cell is to remove the stop words, numbers and punctuation, because this classification is based on the bag of words. Then, it will print the first review to see the filtered words. Then, it will print the first review to see the filtered words."
   ],
   "metadata": {
    "id": "MjGn0Lj5o7ok"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "gvFu36xZ2XT5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a0d229fa-010a-4bcf-8553-f39049d75c38"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['one', 'thing', 'common', 'hollywood', 'major', 'studios', 'productions', 'moving', 'toward', 'mainstream', 'although', 'twentieth', 'century', 'fox', 'new', 'line', 'cinema', 'spawned', 'subsidiaries', 'specialize', 'independant', 'controversial', 'motion', 'pictures', 'fox', 'searchlight', 'fine', 'line', 'respectively', 'obvious', 'significant', 'movement', 'underway', 'promote', 'inventive', 'ideas', 'theater', 'movie', 'like', 'gary', 'ross', 'pleasantville', 'comes', 'along', 'wrapped', 'blanket', 'innovative', 'ideas', 'served', 'platter', 'fine', 'production', 'welcome', 'change', 'pace', 'frequent', 'cineplexes', 'although', 'atmosphere', 'buzz', 'movie', 'cheery', 'lighthearted', 'pleasantville', 'mistaken', 'thought', 'movie', 'quite', 'opposite', 'true', 'fact', 'director', 'ross', 'skillfully', 'brings', 'narrative', 'intense', 'intelligent', 'undertones', 'screen', 'story', 'joys', 'living', 'life', 'fullest', 'well', 'social', 'ills', 'segregation', 'captures', 'essence', 'statement', 'making', 'cinema', 'recent', 'memory', 'movie', 'carried', 'weight', 'theme', 'widescale', 'distribution', 'even', 'mass', 'audiences', 'fail', 'see', 'ingenuity', 'ross', 'work', 'still', 'testament', 'picture', 'considered', 'worthy', 'enough', 'kudos', 'production', 'staff', 'putting', 'together', 'fine', 'picture', 'stars', 'tobey', 'maguire', 'reese', 'witherspoon', 'two', 'nineties', 'teenagers', 'quite', 'bit', 'luck', 'find', 'zapped', 'fifties', 'sitcom', 'named', 'pleasantville', 'maguire', 'reserved', 'bud', 'parker', 'perfect', 'pseudo', 'world', 'rebellious', 'mary', 'sue', 'witherspoon', 'made', 'father', 'knows', 'best', 'times', 'long', 'sets', 'town', 'pleasantville', 'end', 'begins', 'teach', 'townspeople', 'quite', 'unlikely', 'way', 'life', 'really', 'like', 'soon', 'hues', 'color', 'creep', 'black', 'white', 'world', 'embrace', 'change', 'passion', 'realism', 'others', 'fear', 'strangeness', 'one', 'end', 'soda', 'shop', 'owner', 'turned', 'painter', 'named', 'mr', 'johnson', 'jeff', 'daniels', 'end', 'close', 'minded', 'mayor', 'j', 'walsh', 'chamber', 'commerce', 'two', 'sides', 'simply', 'fronts', 'clash', 'ideas', 'soon', 'involves', 'entire', 'town', 'fine', 'performances', 'turned', 'around', 'maguire', 'witherspoon', 'effective', 'leads', 'true', 'complements', 'go', 'jeff', 'daniels', 'j', 'walsh', 'latest', 'final', 'posthumous', 'performance', 'daniels', 'brings', 'atmosphere', 'awe', 'inspiration', 'hope', 'character', 'whereas', 'walsh', 'simply', 'drips', 'sinister', 'closemindedness', 'almost', 'point', 'sense', 'gene', 'hackman', 'envy', 'daniels', 'commendable', 'performance', 'last', 'three', 'years', 'walsh', 'complementary', 'exclamation', 'point', 'fine', 'career', 'also', 'lesser', 'note', 'supporting', 'role', 'joan', 'allen', 'viewing', 'entire', 'cast', 'works', 'well', 'together', 'start', 'finish', 'much', 'dislike', 'pleasantville', 'dynamic', 'nature', 'makes', 'perfect', 'movie', 'want', 'serious', 'well']\n",
      "['every', 'reviewers', 'faced', 'films', 'hard', 'properly', 'reviewed', 'time', 'happens', 'films', 'leave', 'overwhelming', 'impact', 'either', 'good', 'bad', 'end', 'reviewers', 'must', 'work', 'hard', 'express', 'thoughts', 'feelings', 'sometimes', 'happen', 'rather', 'trivial', 'reasons', 'love', 'trouble', 'happened', 'one', 'occasions', 'author', 'review', 'impression', 'left', 'film', 'overwhelming', 'contrary', 'hardly', 'impression', 'since', 'real', 'trouble', 'keeping', 'awake', 'watching', 'surprise', 'day', 'movie', 'theatre', 'full', 'close', 'sound', 'speakers', 'show', 'late', 'lack', 'sleep', 'show', 'things', 'happen', 'rarely', 'many', 'years', 'closest', 'thing', 'solution', 'mystery', 'probably', 'quality', 'film', 'plot', 'revolves', 'around', 'two', 'rival', 'chicago', 'reporters', 'old', 'peter', 'brackett', 'nick', 'nolte', 'young', 'aspiring', 'sabrina', 'peterson', 'julia', 'roberts', 'two', 'assigned', 'cover', 'train', 'collision', 'soon', 'meet', 'start', 'scooping', 'process', 'discover', 'sinister', 'plot', 'involving', 'cancerogenic', 'milk', 'also', 'romantic', 'feelings', 'plot', 'film', 'rather', 'secondary', 'real', 'raison', 'tre', 'romantic', 'pairing', 'reminiscent', 'classical', 'screwball', 'comedies', 'starring', 'spencer', 'tracy', 'katharine', 'hepburn', 'movie', 'author', 'director', 'screenwriter', 'charles', 'shyer', 'experiences', 'turning', 'screwball', 'spirit', 'modern', 'setting', 'father', 'bride', 'tries', 'time', 'pairing', 'old', 'nick', 'nolte', 'young', 'julia', 'roberts', 'however', 'although', 'chemistry', 'two', 'soon', 'stops', 'arouse', 'interest', 'probably', 'happens', 'due', 'poorly', 'executed', 'genre', 'mix', 'collides', 'light', 'hearted', 'romantic', 'comedy', 'rather', 'uninteresting', 'plot', 'suitable', 'pure', 'action', 'thrillers', 'shyer', 'director', 'fails', 'make', 'proper', 'transition', 'two', 'fails', 'areas', 'making', 'story', 'cliched', 'predictable', 'end', 'result', 'rather', 'forgettable', 'effort', 'convinced', 'watch', 'movie', 'give', 'benefit', 'doubt', 'though']\n"
     ]
    }
   ],
   "source": [
    "def filter_stopwords(review):\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    filter_data = []\n",
    "    for i, file in enumerate(review):\n",
    "        filter_data.append([])\n",
    "        for w in file:\n",
    "            if w not in stop_words and w.isalpha():\n",
    "                filter_data[i].append(w)\n",
    "    return filter_data\n",
    "\n",
    "pos_training_data = filter_stopwords(pos_training_data)\n",
    "neg_training_data = filter_stopwords(neg_training_data)\n",
    "\n",
    "print(pos_training_data[0])\n",
    "print(neg_training_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "As the requirements, we need to create two bags of words with ten positive words and ten negative words.  Generally speaking, adjectives or adverbs usually more easily show stronger feelings than some nouns or verbs. Thus, I need to download postagger and tagger every word in all reviews, and then make them in the tuple.   "
   ],
   "metadata": {
    "id": "CGOFTLFRpo8l"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "RgePc9hV2XT_",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d3c570be-4702-4194-d008-2867e69428c3"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('one', 'CD'), ('thing', 'NN'), ('common', 'JJ'), ('hollywood', 'NN'), ('major', 'JJ'), ('studios', 'NNS'), ('productions', 'NNS'), ('moving', 'VBG'), ('toward', 'IN'), ('mainstream', 'JJ'), ('although', 'IN'), ('twentieth', 'JJ'), ('century', 'NN'), ('fox', 'VBD'), ('new', 'JJ'), ('line', 'NN'), ('cinema', 'NN'), ('spawned', 'VBD'), ('subsidiaries', 'NNS'), ('specialize', 'VBP'), ('independant', 'JJ'), ('controversial', 'JJ'), ('motion', 'NN'), ('pictures', 'NNS'), ('fox', 'VBP'), ('searchlight', 'JJ'), ('fine', 'JJ'), ('line', 'NN'), ('respectively', 'RB'), ('obvious', 'JJ'), ('significant', 'JJ'), ('movement', 'NN'), ('underway', 'JJ'), ('promote', 'JJ'), ('inventive', 'JJ'), ('ideas', 'NNS'), ('theater', 'VBP'), ('movie', 'NN'), ('like', 'IN'), ('gary', 'JJ'), ('ross', 'NN'), ('pleasantville', 'NN'), ('comes', 'VBZ'), ('along', 'RB'), ('wrapped', 'JJ'), ('blanket', 'NN'), ('innovative', 'JJ'), ('ideas', 'NNS'), ('served', 'VBD'), ('platter', 'JJ'), ('fine', 'JJ'), ('production', 'NN'), ('welcome', 'NN'), ('change', 'NN'), ('pace', 'NN'), ('frequent', 'JJ'), ('cineplexes', 'NNS'), ('although', 'IN'), ('atmosphere', 'JJ'), ('buzz', 'JJ'), ('movie', 'NN'), ('cheery', 'NN'), ('lighthearted', 'VBD'), ('pleasantville', 'JJ'), ('mistaken', 'NNS'), ('thought', 'VBD'), ('movie', 'NN'), ('quite', 'RB'), ('opposite', 'JJ'), ('true', 'JJ'), ('fact', 'NN'), ('director', 'NN'), ('ross', 'NN'), ('skillfully', 'RB'), ('brings', 'VBZ'), ('narrative', 'JJ'), ('intense', 'JJ'), ('intelligent', 'NN'), ('undertones', 'NNS'), ('screen', 'JJ'), ('story', 'NN'), ('joys', 'NN'), ('living', 'VBG'), ('life', 'NN'), ('fullest', 'JJS'), ('well', 'RB'), ('social', 'JJ'), ('ills', 'NNS'), ('segregation', 'NN'), ('captures', 'VBZ'), ('essence', 'JJ'), ('statement', 'NN'), ('making', 'NN'), ('cinema', 'JJ'), ('recent', 'JJ'), ('memory', 'NN'), ('movie', 'NN'), ('carried', 'VBD'), ('weight', 'JJ'), ('theme', 'NN'), ('widescale', 'NN'), ('distribution', 'NN'), ('even', 'RB'), ('mass', 'VBD'), ('audiences', 'NNS'), ('fail', 'VBP'), ('see', 'VBP'), ('ingenuity', 'JJ'), ('ross', 'NN'), ('work', 'NN'), ('still', 'RB'), ('testament', 'JJ'), ('picture', 'NN'), ('considered', 'VBN'), ('worthy', 'JJ'), ('enough', 'RB'), ('kudos', 'JJ'), ('production', 'NN'), ('staff', 'NN'), ('putting', 'VBG'), ('together', 'RB'), ('fine', 'JJ'), ('picture', 'NN'), ('stars', 'VBZ'), ('tobey', 'JJ'), ('maguire', 'NN'), ('reese', 'JJ'), ('witherspoon', 'NN'), ('two', 'CD'), ('nineties', 'NNS'), ('teenagers', 'NNS'), ('quite', 'RB'), ('bit', 'VBP'), ('luck', 'JJ'), ('find', 'VBP'), ('zapped', 'JJ'), ('fifties', 'NNS'), ('sitcom', 'VBP'), ('named', 'VBN'), ('pleasantville', 'NN'), ('maguire', 'NN'), ('reserved', 'VBD'), ('bud', 'JJ'), ('parker', 'NN'), ('perfect', 'NN'), ('pseudo', 'NN'), ('world', 'NN'), ('rebellious', 'JJ'), ('mary', 'JJ'), ('sue', 'NN'), ('witherspoon', 'NN'), ('made', 'VBD'), ('father', 'NN'), ('knows', 'VBZ'), ('best', 'RBS'), ('times', 'NNS'), ('long', 'JJ'), ('sets', 'NNS'), ('town', 'NN'), ('pleasantville', 'JJ'), ('end', 'NN'), ('begins', 'VBZ'), ('teach', 'JJ'), ('townspeople', 'JJ'), ('quite', 'RB'), ('unlikely', 'JJ'), ('way', 'NN'), ('life', 'NN'), ('really', 'RB'), ('like', 'JJ'), ('soon', 'RB'), ('hues', 'NNS'), ('color', 'VBP'), ('creep', 'JJ'), ('black', 'JJ'), ('white', 'JJ'), ('world', 'NN'), ('embrace', 'NN'), ('change', 'NN'), ('passion', 'NN'), ('realism', 'NN'), ('others', 'NNS'), ('fear', 'VBP'), ('strangeness', 'RB'), ('one', 'CD'), ('end', 'NN'), ('soda', 'NN'), ('shop', 'NN'), ('owner', 'NN'), ('turned', 'VBD'), ('painter', 'RB'), ('named', 'VBN'), ('mr', 'NN'), ('johnson', 'NN'), ('jeff', 'NN'), ('daniels', 'NNS'), ('end', 'VBP'), ('close', 'RB'), ('minded', 'JJ'), ('mayor', 'NN'), ('j', 'NN'), ('walsh', 'JJ'), ('chamber', 'NN'), ('commerce', 'NN'), ('two', 'CD'), ('sides', 'NNS'), ('simply', 'RB'), ('fronts', 'NNS'), ('clash', 'VBP'), ('ideas', 'NNS'), ('soon', 'RB'), ('involves', 'VBZ'), ('entire', 'JJ'), ('town', 'NN'), ('fine', 'NN'), ('performances', 'NNS'), ('turned', 'VBD'), ('around', 'RP'), ('maguire', 'JJ'), ('witherspoon', 'NN'), ('effective', 'JJ'), ('leads', 'VBZ'), ('true', 'JJ'), ('complements', 'NNS'), ('go', 'VB'), ('jeff', 'JJ'), ('daniels', 'NNS'), ('j', 'VBP'), ('walsh', 'JJ'), ('latest', 'JJS'), ('final', 'JJ'), ('posthumous', 'JJ'), ('performance', 'NN'), ('daniels', 'NNS'), ('brings', 'VBZ'), ('atmosphere', 'RB'), ('awe', 'JJ'), ('inspiration', 'NN'), ('hope', 'VBP'), ('character', 'NN'), ('whereas', 'NNS'), ('walsh', 'VBP'), ('simply', 'RB'), ('drips', 'JJ'), ('sinister', 'NN'), ('closemindedness', 'NN'), ('almost', 'RB'), ('point', 'NN'), ('sense', 'NN'), ('gene', 'NN'), ('hackman', 'NN'), ('envy', 'NN'), ('daniels', 'NNS'), ('commendable', 'JJ'), ('performance', 'NN'), ('last', 'JJ'), ('three', 'CD'), ('years', 'NNS'), ('walsh', 'VBP'), ('complementary', 'JJ'), ('exclamation', 'NN'), ('point', 'NN'), ('fine', 'JJ'), ('career', 'NN'), ('also', 'RB'), ('lesser', 'VBD'), ('note', 'NN'), ('supporting', 'VBG'), ('role', 'NN'), ('joan', 'NN'), ('allen', 'IN'), ('viewing', 'VBG'), ('entire', 'JJ'), ('cast', 'NN'), ('works', 'NNS'), ('well', 'RB'), ('together', 'RB'), ('start', 'VB'), ('finish', 'VB'), ('much', 'RB'), ('dislike', 'IN'), ('pleasantville', 'JJ'), ('dynamic', 'JJ'), ('nature', 'NN'), ('makes', 'VBZ'), ('perfect', 'JJ'), ('movie', 'NN'), ('want', 'VBP'), ('serious', 'JJ'), ('well', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def pos_tagged(review):\n",
    "    tagged_data = []\n",
    "    for file in review:\n",
    "        tagged_data.append(nltk.pos_tag(file))\n",
    "    return tagged_data\n",
    "\n",
    "pos_tagged_data = pos_tagged(pos_training_data)\n",
    "neg_tagged_data = pos_tagged(neg_training_data)\n",
    "print(pos_tagged_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "On the table, adjective is equal with JJ. I have selected all adjective words. Now I will print the first ten words to show the example.  "
   ],
   "metadata": {
    "id": "vVppVmgQc6rx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def adj_words(review):\n",
    "    words = []\n",
    "    for file in review:\n",
    "        for w in file:\n",
    "            if w[1] == \"JJ\":\n",
    "                words.append(w[0])\n",
    "    return words\n",
    "\n",
    "pos_selected_words = adj_words(pos_tagged_data)\n",
    "neg_selected_words = adj_words(neg_tagged_data)\n",
    "\n",
    "print(pos_selected_words[:10])"
   ],
   "metadata": {
    "id": "pRyp6-0iemQQ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5ae731ca-0605-4459-b54e-a23bfd42a679"
   },
   "execution_count": 34,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['common', 'major', 'mainstream', 'twentieth', 'new', 'independant', 'controversial', 'searchlight', 'fine', 'obvious']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The function of FreqDist will return a structure similar as dictionary, and the values are the occurred times of these words. Here I will pick up the most common 100 words from more to less."
   ],
   "metadata": {
    "id": "UGHFoSReYJjD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def words_times(words, k):\n",
    "    doc = nltk.FreqDist(words)\n",
    "    word_time = []\n",
    "    for w in doc.most_common(k):\n",
    "        word_time.append(w)\n",
    "    return word_time\n",
    "\n",
    "pos_word = list(set(pos_selected_words))\n",
    "neg_word = list(set(neg_selected_words))\n",
    "\n",
    "pos_times = words_times(pos_selected_words, 100)\n",
    "neg_times = words_times(neg_selected_words, 100)\n",
    "\n",
    "print(pos_times)\n",
    "print(neg_times)"
   ],
   "metadata": {
    "id": "yyCv73zzemXF",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f2844a0d-9ddf-4129-a7ce-d8c123874bda"
   },
   "execution_count": 35,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('good', 891), ('many', 562), ('great', 539), ('much', 522), ('little', 512), ('new', 494), ('first', 395), ('real', 342), ('old', 339), ('big', 304), ('young', 300), ('last', 295), ('american', 263), ('original', 252), ('bad', 244), ('funny', 226), ('true', 220), ('high', 219), ('black', 211), ('special', 210), ('small', 187), ('several', 183), ('different', 182), ('right', 182), ('sure', 173), ('human', 169), ('final', 158), ('hard', 157), ('entire', 156), ('whole', 153), ('main', 153), ('comic', 150), ('second', 149), ('enough', 145), ('full', 141), ('able', 141), ('next', 137), ('long', 133), ('strong', 132), ('beautiful', 131), ('dead', 129), ('short', 127), ('screen', 124), ('top', 123), ('classic', 123), ('nice', 122), ('white', 120), ('perfect', 119), ('fine', 114), ('wonderful', 113), ('romantic', 111), ('evil', 110), ('wrong', 109), ('major', 108), ('excellent', 107), ('hilarious', 105), ('deep', 104), ('effective', 102), ('interesting', 101), ('live', 101), ('emotional', 99), ('simple', 97), ('happy', 97), ('due', 95), ('alien', 94), ('personal', 94), ('powerful', 94), ('dark', 93), ('titanic', 92), ('political', 90), ('important', 88), ('previous', 87), ('recent', 86), ('local', 86), ('impressive', 85), ('visual', 84), ('similar', 84), ('possible', 84), ('certain', 84), ('obvious', 83), ('difficult', 83), ('early', 82), ('solid', 82), ('single', 80), ('memorable', 80), ('easy', 77), ('former', 77), ('present', 77), ('overall', 76), ('private', 76), ('sweet', 76), ('rich', 75), ('popular', 75), ('dramatic', 75), ('brilliant', 73), ('general', 72), ('subtle', 72), ('future', 71), ('serious', 70), ('familiar', 70)]\n",
      "[('good', 816), ('bad', 734), ('much', 504), ('little', 477), ('big', 435), ('new', 391), ('many', 343), ('first', 302), ('old', 293), ('great', 293), ('real', 272), ('last', 272), ('funny', 252), ('original', 250), ('young', 235), ('high', 189), ('special', 181), ('whole', 180), ('hard', 157), ('sure', 157), ('black', 146), ('wrong', 145), ('enough', 141), ('interesting', 140), ('main', 138), ('dead', 136), ('next', 132), ('stupid', 131), ('long', 131), ('second', 131), ('right', 126), ('obvious', 122), ('full', 121), ('human', 119), ('american', 117), ('entire', 113), ('nice', 110), ('several', 108), ('small', 108), ('final', 106), ('poor', 106), ('different', 106), ('comic', 105), ('top', 103), ('screen', 100), ('short', 100), ('evil', 98), ('able', 95), ('true', 94), ('live', 92), ('early', 91), ('romantic', 90), ('single', 89), ('possible', 88), ('predictable', 87), ('major', 85), ('wild', 84), ('alien', 84), ('white', 82), ('ridiculous', 82), ('recent', 79), ('serious', 79), ('terrible', 77), ('awful', 77), ('give', 74), ('late', 73), ('complete', 72), ('familiar', 72), ('deep', 71), ('popular', 71), ('classic', 71), ('sexual', 71), ('scary', 70), ('general', 69), ('large', 68), ('know', 67), ('close', 66), ('low', 66), ('huge', 66), ('potential', 64), ('local', 64), ('decent', 63), ('emotional', 63), ('dumb', 63), ('fine', 62), ('middle', 62), ('difficult', 62), ('third', 61), ('dull', 61), ('certain', 61), ('worth', 59), ('strong', 59), ('important', 59), ('impossible', 59), ('beautiful', 58), ('cool', 58), ('nick', 57), ('particular', 57), ('slow', 57), ('simple', 57)]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bacially, it cannot be defined the positive words or negative words only based on times, because some words such as good, it occured 891 times in positive reviews, however, it was also in negative reviews with the same times. I will ingore the words such as these, and I will only choose those words which are only in positive reviews or negative reviews, rather than in another one."
   ],
   "metadata": {
    "id": "oKRMv2UyoEdp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def words_select(select_words, sample_words):\n",
    "    filter_words = []\n",
    "    for w in select_words:\n",
    "        isPrime = 1\n",
    "        for word in sample_words:\n",
    "            if w[0] == word[0]:\n",
    "                isPrime = 0\n",
    "                break\n",
    "        if isPrime == 1:\n",
    "            filter_words.append(w)\n",
    "    return filter_words\n",
    "\n",
    "pos_words = words_select(pos_times, neg_times)\n",
    "neg_words = words_select(neg_times, pos_times)\n",
    "\n",
    "print(pos_words)\n",
    "print(neg_words)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YzfsnYXcZtwW",
    "outputId": "cdc8c158-7588-486d-acb0-a74c6a4d69bf"
   },
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('perfect', 119), ('wonderful', 113), ('excellent', 107), ('hilarious', 105), ('effective', 102), ('happy', 97), ('due', 95), ('personal', 94), ('powerful', 94), ('dark', 93), ('titanic', 92), ('political', 90), ('previous', 87), ('impressive', 85), ('visual', 84), ('similar', 84), ('solid', 82), ('memorable', 80), ('easy', 77), ('former', 77), ('present', 77), ('overall', 76), ('private', 76), ('sweet', 76), ('rich', 75), ('dramatic', 75), ('brilliant', 73), ('subtle', 72), ('future', 71)]\n",
      "[('stupid', 131), ('poor', 106), ('predictable', 87), ('wild', 84), ('ridiculous', 82), ('terrible', 77), ('awful', 77), ('give', 74), ('late', 73), ('complete', 72), ('sexual', 71), ('scary', 70), ('large', 68), ('know', 67), ('close', 66), ('low', 66), ('huge', 66), ('potential', 64), ('decent', 63), ('dumb', 63), ('middle', 62), ('third', 61), ('dull', 61), ('worth', 59), ('impossible', 59), ('cool', 58), ('nick', 57), ('particular', 57), ('slow', 57)]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, the two bags of words have finished. Traditionally, these words are mostly good words or bad words, such as perfect, wonderful, excellent, stupid, terrible, etc."
   ],
   "metadata": {
    "id": "w2QxBzecpLmw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pos_bag = [w[0] for w in pos_words[:10]]\n",
    "neg_bag = [w[0] for w in neg_words[:10]]\n",
    "\n",
    "print(pos_bag)\n",
    "print(neg_bag)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmhmr3OSZt5O",
    "outputId": "3a0d2b76-ba23-445c-ab4b-be749b0ab9e7"
   },
   "execution_count": 37,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['perfect', 'wonderful', 'excellent', 'hilarious', 'effective', 'happy', 'due', 'personal', 'powerful', 'dark']\n",
      "['stupid', 'poor', 'predictable', 'wild', 'ridiculous', 'terrible', 'awful', 'give', 'late', 'complete']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JauTzY5N2XUB"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TApOQE6vND20"
   },
   "source": [
    "2)\n",
    "a) **Use** the lists generated in Q1 to build a **word list classifier** which will classify reviews as being positive or negative.\n",
    "\n",
    "b) **Explain** what you have done.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a word list classifier and I will explain the theory. To start with, I will set up two points equal with zero, which are positive and negative points. Secondly, the words of every reviews was in the loop. If the word is in positive bag, the positive point will plus one, on the contrary the negative point is same. Clearly, if more positive words are in this review, the chance of positive review are more, and the negative words are same. Only one case is that they are equal, it will random to guess, however it is rare."
   ],
   "metadata": {
    "id": "IQg1ry6cuGsG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "BThDMrcmODJy"
   },
   "outputs": [],
   "source": [
    "class BasicClassification:\n",
    "    def __init__(self, content):\n",
    "        self.content = content\n",
    "        self.pos = 0\n",
    "        self.neg = 0\n",
    "\n",
    "    def classify(self, pos_bag, neg_bag):\n",
    "        for w in self.content:\n",
    "            if w in pos_bag:\n",
    "                self.pos += 1\n",
    "            if w in neg_bag:\n",
    "                self.neg += 1\n",
    "        if self.pos > self.neg:\n",
    "            return 1\n",
    "        if self.pos < self.neg:\n",
    "            return 0\n",
    "        else:\n",
    "            return random.randint(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "I start to do the test for the testing data. I make two list collecting the return result, and the positive predict is one, and the negative predict is zero."
   ],
   "metadata": {
    "id": "uCzJgsPaxFow"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pos_result = []\n",
    "for file in pos_testing_data:\n",
    "    rst = BasicClassification(file)\n",
    "    pos_result.append(rst.classify(pos_bag, neg_bag))\n",
    "neg_result = []\n",
    "for file in neg_testing_data:\n",
    "    rst = BasicClassification(file)\n",
    "    neg_result.append(rst.classify(pos_bag, neg_bag))\n",
    "\n",
    "print(pos_result[:20])\n",
    "print(neg_result[:20])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Sma9Onmaodn",
    "outputId": "25389775-9312-4120-b1d6-d33831e87c5a"
   },
   "execution_count": 39,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6vK5Vyz2XUF"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZdDO_Y92XUH"
   },
   "source": [
    "3)\n",
    "a) **Calculate** the accuracy, precision, recall and F1 score of your classifier.\n",
    "\n",
    "b) Is it reasonable to evaluate the classifier in terms of its accuracy?  **Explain** your answer and give a counter-example (a scenario where it would / would not be reasonable to evaluate the classifier in terms of its accuracy).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "I respectively calculate the recall rate, precision, accuracy and error rate. I will print and analyse the data with my explaination and some understanding. It is not difficult to find that the data was very average, which is the good part of this experiment. It is not happened that the accuracy was high but the recall rate or precision were very low. Every value is about 60 percent. Perhaps it is not very ideal, but it is an acceptable consequence, because it is not below 50 percent, which means that you messed it up.  "
   ],
   "metadata": {
    "id": "9OZpicjlzD-v"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pos_recall_rate = pos_result.count(1) / len(pos_result)\n",
    "neg_recall_rate = neg_result.count(0) / len(neg_result)\n",
    "pos_precision = pos_result.count(1) / (pos_result.count(1) + neg_result.count(1))\n",
    "neg_precision = neg_result.count(0) / (pos_result.count(0) + neg_result.count(0))\n",
    "accuracy = (pos_result.count(1) + neg_result.count(0)) / len(pos_result + neg_result)\n",
    "error_rate = (pos_result.count(0) + neg_result.count(1)) / len(pos_result + neg_result)"
   ],
   "metadata": {
    "id": "9r-yM-kwa2Ll"
   },
   "execution_count": 40,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df1 = pd.DataFrame([[pos_result.count(1), pos_result.count(0), len(pos_result)],\n",
    "                     [neg_result.count(1), neg_result.count(0), len(neg_result)],\n",
    "                     [pos_result.count(1) + neg_result.count(1), pos_result.count(0) + neg_result.count(0),\n",
    "                      len(pos_result + neg_result)]], index=(\"+ve\", \"-ve\", \"total\"), columns=(\"+ve\", \"-ve\", \"total\"))\n",
    "\n",
    "print(df1)"
   ],
   "metadata": {
    "id": "qKZ7czUVaon9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "63c4a100-3840-4137-e3fc-4d8d12a55259"
   },
   "execution_count": 41,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "       +ve  -ve  total\n",
      "+ve    191  109    300\n",
      "-ve    102  198    300\n",
      "total  293  307    600\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"pos_recall_rate = {:.2f}\". format(pos_recall_rate))\n",
    "print(\"neg_recall_rate = {:.2f}\". format(neg_recall_rate))\n",
    "print(\"pos_precision = {:.2f}\". format(pos_precision))\n",
    "print(\"neg_precision = {:.2f}\". format(neg_precision))\n",
    "print(\"accuracy = {:.2f}\". format(accuracy))\n",
    "print(\"error_rate = {:.2f}\". format(error_rate))"
   ],
   "metadata": {
    "id": "YNVDSDuFao8N",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d388de0d-004f-4215-e48a-618c3c87f2c1"
   },
   "execution_count": 42,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pos_recall_rate = 0.64\n",
      "neg_recall_rate = 0.66\n",
      "pos_precision = 0.65\n",
      "neg_precision = 0.64\n",
      "accuracy = 0.65\n",
      "error_rate = 0.35\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The advantage is that the theory is quite simple and also the code is not too complicated to implement, which means that you do not need the professionals or too high maintenance cost. It is not easy to read the code written by another one, especially when the code is difficult and longer. Secondly, the data results are stable. Even though it is probably not the best method, at least the stable results show that you did not make clear mistakes during the process. Additionally, the experimental data shows no significant bias, and I will show this problem the Naive Bayes classifier for next.  \n",
    "\n",
    "However, the weakness is obvious as well. The first one is that no matter what the precision, recall rate or even accuracy, they are not very high. Moreover, it is same as Bayes classifier, and both of them need the specifical condition, which is the independence of each word in the paper. However, it is usually impossible. Thirdly, it is a massive task to build up the bag of words, and it depends on the amount of data set or some else situations, but here it was just simplified, which is not enough. Finally, the setting of the K value significantly decided the result of trail. It means the values between the times in positive reviews or negative reviews were subtracted or divided, after that the confirmed K value started important impact on the consequence."
   ],
   "metadata": {
    "id": "DQM-cMRBD2BH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df2 = pd.DataFrame([[0.55, 0.67, 0.60, 0.69, 0.54, 0.65, 0.66, 0.54, 0.65, 0.63],\n",
    "                    [0.71, 0.61, 0.65, 0.62, 0.71, 0.62, 0.55, 0.67, 0.70, 0.69],\n",
    "                    [0.66, 0.63, 0.63, 0.65, 0.65, 0.63, 0.61, 0.62, 0.68, 0.67],\n",
    "                    [0.61, 0.65, 0.62, 0.67, 0.61, 0.64, 0.63, 0.59, 0.66, 0.65],\n",
    "                    [0.63, 0.64, 0.62, 0.66, 0.62, 0.64, 0.61, 0.60, 0.66, 0.65],\n",
    "                    [0.37, 0.36, 0.38, 0.34, 0.38, 0.36, 0.39, 0.40, 0.33, 0.34]],\n",
    "                   index=(\"pos recall rate\", \"neg recall rate\", \"pos precision\", \"neg precision\", \"accuracy\", \"error rate\"),\n",
    "                   columns=(\"round1\", \"round2\", \"round3\", \"round4\", \"round5\", \"round6\", \"round7\", \"round8\", \"round9\", \"round10\"))\n",
    "\n",
    "print(df2)"
   ],
   "metadata": {
    "id": "oQpMF-mJapCl",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ba97f2c2-f1a7-4053-9d36-836e919ecef2"
   },
   "execution_count": 43,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                 round1  round2  round3  round4  round5  round6  round7  \\\n",
      "pos recall rate    0.55    0.67    0.60    0.69    0.54    0.65    0.66   \n",
      "neg recall rate    0.71    0.61    0.65    0.62    0.71    0.62    0.55   \n",
      "pos precision      0.66    0.63    0.63    0.65    0.65    0.63    0.61   \n",
      "neg precision      0.61    0.65    0.62    0.67    0.61    0.64    0.63   \n",
      "accuracy           0.63    0.64    0.62    0.66    0.62    0.64    0.61   \n",
      "error rate         0.37    0.36    0.38    0.34    0.38    0.36    0.39   \n",
      "\n",
      "                 round8  round9  round10  \n",
      "pos recall rate    0.54    0.65     0.63  \n",
      "neg recall rate    0.67    0.70     0.69  \n",
      "pos precision      0.62    0.68     0.67  \n",
      "neg precision      0.59    0.66     0.65  \n",
      "accuracy           0.60    0.66     0.65  \n",
      "error rate         0.40    0.33     0.34  \n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVZp0N5J2XUL"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIS9UpmJNEAp"
   },
   "source": [
    "4)\n",
    "a)  **Construct** a Naive Bayes classifier (e.g., from NLTK).\n",
    "\n",
    "b)  **Compare** the performance of your word list classifier with the Naive Bayes classifier.  **Discuss** your results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some parts of Bayers classifier are similar as the BOW classifier, such as filtering the stop words and building all words list. Some are different, for example, the part of calculating the probilities. Before starting the explanation, I need to list the Bayers formula here, because I will explain based on the elements of this formula.\n",
    "   "
   ],
   "metadata": {
    "id": "ldGLoyofToyt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def words_list(reviews):\n",
    "    all_words = []\n",
    "    for review in reviews:\n",
    "        all_words += review\n",
    "    return all_words\n",
    "\n"
   ],
   "metadata": {
    "id": "YTxS321Qn3So"
   },
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pos_words = words_list(pos_training_data)\n",
    "neg_words = words_list(neg_training_data)\n",
    "\n",
    "pos_testing_data = filter_stopwords(pos_testing_data)\n",
    "neg_testing_data = filter_stopwords(neg_testing_data)\n",
    "pos_testing_words = words_list(pos_testing_data)\n",
    "neg_testing_words = words_list(neg_testing_data)"
   ],
   "metadata": {
    "id": "SXt_3jqmn3MK"
   },
   "execution_count": 45,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "note: it probably needs two mintues"
   ],
   "metadata": {
    "id": "16ahj8cXAHvo"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "Gwjig-Y12XUN"
   },
   "outputs": [],
   "source": [
    "def words_final(words, test):\n",
    "    words += list(set(words))\n",
    "    for w in set(test):\n",
    "        if w not in words:\n",
    "            words.append(w)\n",
    "    return words\n",
    "\n",
    "pos_words = words_final(pos_words, pos_testing_words + neg_testing_words)\n",
    "neg_words = words_final(neg_words, pos_testing_words + neg_testing_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "3AUsYRMa2XUN"
   },
   "outputs": [],
   "source": [
    "pos_doc = nltk.FreqDist(pos_words)\n",
    "neg_doc = nltk.FreqDist(neg_words)\n",
    "pos_len = len(pos_words)\n",
    "neg_len = len(neg_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**P(A|B)=P(B|A)*P(A)/P(B)**: P(B|A) is likelihood, and P(A|B) is posterior. It means that when you get the likelihood and you can get the posterior. P(B|A)*P(A) is equal to P(AB), which is that event A is happened and same time event B is happened as well. It becomes a conditional probability.\n",
    "\n",
    "In this formula, we can assume that B is the destination, and A are the different roads that you can access to B. Now we need to know once you reach B, which road you have taken. In the training data set, the result of the positive review or negative review is event B similar as the destination. Because the number of positive and negative reviews are the same, they are both equal with 0.5. All the words in positive review or negative review are the different roads here, which is Ai (i = 1, 2, 3, …, k). The probability is the probability (occurrence times of the word divided by all numbers of positive or negative words) are divided by the probability of a positive or negative review, 0.5 here.\n",
    "\n",
    "To predict in the testing data set, we need to use another formula related to Bayes, law of total probability. P(B) = P(B|Ai)*P(Ai) + … + P(B|Ak)*P(Ak) (i = 1, 2, 3, …, k). Clearly, one is division, and the other is multiplication. Here Ai is all different in this testing review, and P(Ai) shows that we have meet this word in this review. P(B|Ai) is the chance of positive or negative review once we meet this word, and the value P(AiB) (is P(B|Ai)*P(Ai)) is what we got in training data set before. However, it needs to be assumed the condition, and B is positive or negative. Finally, we compare with the two probabilities when B is positive or negative, and the bigger one is the result which we will predict. To prevent the words are not occurred in training data, which will cause the probability is equal with zero, I used the Laplace smoothing as above code, speaking briefly."
   ],
   "metadata": {
    "id": "peDybjNXbBxG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class BayesClassification:\n",
    "\n",
    "    def __init__(self, content):\n",
    "        self.content = content\n",
    "        self.pos = 0.5\n",
    "        self.neg = 0.5\n",
    "\n",
    "    def predict(self):\n",
    "        pos_possibility = math.log(self.pos, 10)\n",
    "        for w in self.content:\n",
    "            x = math.log(pos_doc[w]/pos_len, 10)\n",
    "            pos_possibility += x\n",
    "        neg_possibility = math.log(self.neg, 10)\n",
    "        for w in self.content:\n",
    "            x = math.log(neg_doc[w]/neg_len, 10)\n",
    "            neg_possibility += x\n",
    "        if pos_possibility > neg_possibility:\n",
    "            return 1\n",
    "        elif pos_possibility < neg_possibility:\n",
    "            return 0\n",
    "        else:\n",
    "            return random.randint(0, 1)"
   ],
   "metadata": {
    "id": "ZvLesJfUoM_A"
   },
   "execution_count": 48,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pos_result = []\n",
    "for review in pos_testing_data:\n",
    "    file = BayesClassification(review)\n",
    "    pos_result.append(file.predict())\n",
    "neg_result = []\n",
    "for review in neg_testing_data:\n",
    "    file = BayesClassification(review)\n",
    "    neg_result.append(file.predict())"
   ],
   "metadata": {
    "id": "fp4qAGMUoM6n"
   },
   "execution_count": 49,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pos_recall_rate = pos_result.count(1) / len(pos_result)\n",
    "neg_recall_rate = neg_result.count(0) / len(neg_result)\n",
    "pos_precision = pos_result.count(1) / (pos_result.count(1) + neg_result.count(1))\n",
    "neg_precision = neg_result.count(0) / (pos_result.count(0) + neg_result.count(0))\n",
    "accuracy = (pos_result.count(1) + neg_result.count(0)) / len(pos_result + neg_result)\n",
    "error_rate = (pos_result.count(0) + neg_result.count(1)) / len(pos_result + neg_result)\n"
   ],
   "metadata": {
    "id": "TWMi5xY_oM1Z"
   },
   "execution_count": 50,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similarly, I will declare the advantages and disadvantages of the Bayes classifier. Obviously, one of advantages is that the accuracy, precision or recall rate are improved compared with the BOW classifier, which overall is about 80 percent. It is an ideal result during ten times of trails as below. Another advantage is it is also not too difficult, because it ignores complicated situations. The third one is that the probability was included in the method or theory, which is more scientifically and professionally.  \n",
    "\n",
    "Howerer, there are still some limitations. Firstly, the basial condition is the independence of every word, but in most cases it is invalid.  For instance, ‘not’ or even ‘!’, these words or even punctuation will totally cause different meanings, or opposite points. Secondly, the accuracy depends on the amount of training data set. Generally, the more complete data sets can usually provide more accurate probabilities, and it can help us get better predictions. I did not change the ratio value in this experiment, so it is one of my speculations. Lastly, only based on this data result, there is an interesting content that the positive recall rate is always lower than negative recall rate. Honestly, it is not a good signal, because it reflects that there must be some unsolved problems. Unfortunately, I have not thought of any reasons or solutions for this issue, such as the length of the text or relevance between every word.  "
   ],
   "metadata": {
    "id": "J145DW4FqlKp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df1 = pd.DataFrame([[pos_result.count(1), pos_result.count(0), len(pos_result)],\n",
    "                     [neg_result.count(1), neg_result.count(0), len(neg_result)],\n",
    "                     [pos_result.count(1) + neg_result.count(1), pos_result.count(0) + neg_result.count(0),\n",
    "                      len(pos_result + neg_result)]], index=(\"+ve\", \"-ve\", \"total\"), columns=(\"+ve\", \"-ve\", \"total\"))\n",
    "print(df1)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AneBxFh0yStg",
    "outputId": "e3ab88a8-43f0-4ee8-85d2-b198cb7ab78b"
   },
   "execution_count": 51,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "       +ve  -ve  total\n",
      "+ve    231   69    300\n",
      "-ve     46  254    300\n",
      "total  277  323    600\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"pos_recall_rate = {:.2f}\". format(pos_recall_rate))\n",
    "print(\"neg_recall_rate = {:.2f}\". format(neg_recall_rate))\n",
    "print(\"pos_precision = {:.2f}\". format(pos_precision))\n",
    "print(\"neg_precision = {:.2f}\". format(neg_precision))\n",
    "print(\"accuracy = {:.2f}\". format(accuracy))\n",
    "print(\"error_rate = {:.2f}\". format(error_rate))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cOvpU_FoMwo",
    "outputId": "f6ff0fdd-9c07-49a1-c2c0-d05cc084829d"
   },
   "execution_count": 52,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pos_recall_rate = 0.77\n",
      "neg_recall_rate = 0.85\n",
      "pos_precision = 0.83\n",
      "neg_precision = 0.79\n",
      "accuracy = 0.81\n",
      "error_rate = 0.19\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "df2 = pd.DataFrame([[0.77, 0.81, 0.78, 0.74, 0.74, 0.77, 0.76, 0.83, 0.76, 0.80],\n",
    "                   [0.85, 0.82, 0.83, 0.85, 0.86, 0.84, 0.88, 0.83, 0.81, 0.86],\n",
    "                   [0.83, 0.82, 0.82, 0.83, 0.84, 0.82, 0.87, 0.83, 0.80, 0.85],\n",
    "                   [0.79, 0.81, 0.79, 0.76, 0.77, 0.78, 0.79, 0.83, 0.77, 0.87],\n",
    "                   [0.81, 0.82, 0.81, 0.79, 0.80, 0.80, 0.82, 0.83, 0.78, 0.83],\n",
    "                   [0.19, 0.18, 0.20, 0.21, 0.20, 0.20, 0.18, 0.17, 0.22, 0.17]],\n",
    "                  index=(\"pos recall rate\", \"neg recall rate\", \"pos precision\", \"neg precision\", \"accuracy\", \"error rate\"),\n",
    "                  columns=(\"round1\", \"round2\", \"round3\", \"round4\", \"round5\", \"round6\", \"round7\", \"round8\", \"round9\", \"round10\"))\n",
    "\n",
    "print(df2)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WH-OfS9uoMpY",
    "outputId": "bd3714ad-8fdc-46a7-9ee5-a7a5a4bf9388"
   },
   "execution_count": 53,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                 round1  round2  round3  round4  round5  round6  round7  \\\n",
      "pos recall rate    0.77    0.81    0.78    0.74    0.74    0.77    0.76   \n",
      "neg recall rate    0.85    0.82    0.83    0.85    0.86    0.84    0.88   \n",
      "pos precision      0.83    0.82    0.82    0.83    0.84    0.82    0.87   \n",
      "neg precision      0.79    0.81    0.79    0.76    0.77    0.78    0.79   \n",
      "accuracy           0.81    0.82    0.81    0.79    0.80    0.80    0.82   \n",
      "error rate         0.19    0.18    0.20    0.21    0.20    0.20    0.18   \n",
      "\n",
      "                 round8  round9  round10  \n",
      "pos recall rate    0.83    0.76     0.80  \n",
      "neg recall rate    0.83    0.81     0.86  \n",
      "pos precision      0.83    0.80     0.85  \n",
      "neg precision      0.83    0.77     0.87  \n",
      "accuracy           0.83    0.78     0.83  \n",
      "error rate         0.17    0.22     0.17  \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "8dh6zl1_oMhA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bytPkuHf2XUO"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGDXaVDqOSfY"
   },
   "source": [
    "5)\n",
    "a) Design and **carry out an experiment** into the impact of the **length of the wordlists** on the wordlist classifier.  Make sure you **describe** design decisions in your experiment, include a **graph** of your results and **discuss** your conclusions.\n",
    "\n",
    "b) Would you **recommend** a wordlist classifier or a Naive Bayes classifier for future work in this area?  **Justify** your answer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlxoUthX2XUP"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apparently, one of limitations is the irrelevance of every word in one sentence. Thus, one hypothesis is that the Bayes classifier can perform better in shorter texts than longer texts. I ignore the difference of positive or negative reviews and mix them up, because this is not what I am focusing on, and the length is only one factor. From the data result, it is kind of unexpected, and I do not get the result same as my speculation. I am not able to understand well, and probably there are some mistakes in my code. It seems to be a low-level error, but it should be considered, because as we all know in python, one indentation could cause a different answer or even error.  \n",
    "\n",
    "In summary, I would suggest that Bayes classifier should be a good choice if you do not need to take over quite difficult situations, because it is quick and simple. The values we get are usually not too bad, compared to the BOW classifier. Currently, I can only compare them, and Bayes is the better one. If I get more methods about sentiment analysis in the future, perhaps I will get a new answer."
   ],
   "metadata": {
    "id": "QLs81J7H_Jcf"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "T1L7mZ-k2XUQ"
   },
   "outputs": [],
   "source": [
    "def quick_sort(arr):\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "\n",
    "    pivot = arr[0]\n",
    "    pivot_left = [x for x in arr[1:] if len(x) <= len(pivot)]\n",
    "    pivot_right = [x for x in arr[1:] if len(x) > len(pivot)]\n",
    "\n",
    "    return quick_sort(pivot_left) + [pivot] + quick_sort(pivot_right)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "xFeOWIRm2XUQ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "98a1802a-46ce-48d4-9bdd-46de621846c4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the average length of short reviews = 165\n",
      "the average length of long reviews = 605\n"
     ]
    }
   ],
   "source": [
    "pos_sort_data = quick_sort(pos_testing_data)\n",
    "neg_sort_data = quick_sort(neg_testing_data)\n",
    "\n",
    "short_data = pos_sort_data[:50] + neg_sort_data[:50]\n",
    "long_data = pos_sort_data[250:] + neg_sort_data[250:]\n",
    "\n",
    "len_short_data = [len(text) for text in short_data]\n",
    "len_long_data = [len(text) for text in long_data]\n",
    "\n",
    "print(\"the average length of short reviews = {:d}\". format(sum(len_short_data) // len(len_short_data)))\n",
    "print(\"the average length of long reviews = {:d}\". format(sum(len_long_data) // len(len_long_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "VT82P88M2XUQ"
   },
   "outputs": [],
   "source": [
    "short_result = []\n",
    "for review in short_data:\n",
    "    file = BayesClassification(review)\n",
    "    short_result.append(file.predict())\n",
    "long_result = []\n",
    "for review in long_data:\n",
    "    file = BayesClassification(review)\n",
    "    long_result.append(file.predict())"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "short_accuracy = (short_result[:50].count(1) + short_result[50:].count(0)) / len(short_result)\n",
    "long_accuracy = (long_result[:50].count(1) + long_result[50:].count(0)) / len(long_result)\n",
    "print(\"short accuracy = {:.2f}\". format(short_accuracy))\n",
    "print(\"long accuracy = {:.2f}\". format(long_accuracy))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BHMkc9z-M2Yq",
    "outputId": "ea354419-a61b-4b1f-9505-4edd7daf3ed8"
   },
   "execution_count": 57,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "short accuracy = 0.77\n",
      "long accuracy = 0.84\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "df3 = pd.DataFrame([[160, 164, 176, 163, 161],\n",
    "                   [612, 594, 591, 594, 601],\n",
    "                   [0.78, 0.88, 0.76, 0.74, 0.76],\n",
    "                   [0.88, 0.85, 0.84, 0.90, 0.83]],\n",
    "                  index=(\"average length of short\", \"average length of long\", \"short accuracy\", \"long accuracy\"),\n",
    "                  columns=(\"round1\", \"round2\", \"round3\", \"round4\", \"round5\"))\n"
   ],
   "metadata": {
    "id": "4p8ehaGhM2NK"
   },
   "execution_count": 58,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ym-TGvYS2XUR"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 271
    },
    "id": "34rdlS_iPov6",
    "outputId": "bae6f910-44c5-4ce6-bdcc-44fe56315be1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-9ea4abea94eb>\u001B[0m in \u001B[0;36m<cell line: 12>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0mquestion_count\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m432\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m \u001B[0;32mwith\u001B[0m \u001B[0mio\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'r'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'utf-8'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     13\u001B[0m     \u001B[0mnb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcurrent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'json'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/ANLPassignment2023.ipynb'"
     ]
    }
   ],
   "source": [
    "##This code will word count all of the markdown cells in the notebook saved at filepath\n",
    "\n",
    "import io\n",
    "from nbformat import current\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "filepath=\"/content/drive/MyDrive/Colab Notebooks/ANLPassignment2023.ipynb\"\n",
    "question_count=432\n",
    "\n",
    "with io.open(filepath, 'r', encoding='utf-8') as f:\n",
    "    nb = current.read(f, 'json')\n",
    "\n",
    "word_count = 0\n",
    "for cell in nb.worksheets[0].cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "print(\"Submission length is {}\".format(word_count-question_count))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
