{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#Applied Natural Language Processing 955G5\n",
    "##Computer Based Examination, 2023\n",
    "\n",
    "Remember, you can add cells and change their type (between code and text/markdown) as required to answer the questions."
   ],
   "metadata": {
    "id": "XXTZiUkPpU8i"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# update your candidate number here\n",
    "candidate_number = 11111111"
   ],
   "metadata": {
    "id": "0sHphG0ppUR7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 1 (50 marks)\n",
    "\n",
    "This question is about document similarity and information retrieval."
   ],
   "metadata": {
    "id": "Dyg8LujTcyR_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "J1cyZg-ecD0g",
    "ExecuteTime": {
     "end_time": "2024-01-14T18:07:15.466263Z",
     "start_time": "2024-01-14T18:07:15.459696300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### do not change the code in this cell\n",
    "# make sure you run this cell\n",
    "from math import log, sqrt\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "documents=[[\"NLTK\", \",\", \"or\", \"Natural\", \"Language\", \"Toolkit\" \",\", \"is\", \"a\", \"Python\", \"package\", \"that\", \"you\", \"can\", \"use\", \"for\", \"NLP\", \".\"],\n",
    "           [\"Neuro-linguistic\", \"programming\", \"(\", \"NLP\", \")\", \"is\", \"a\", \"pseudoscientific\", \"approach\", \"to\", \"communication\" \",\", \"personal\", \"development\", \"and\", \"psychotherapy\", \".\"],\n",
    "           [\"Python\", \"is\", \"a\", \"genus\", \"of\", \"constricting\", \"snakes\", \"in\", \"the\", \"Pythonidae\", \"family\", \".\"]]\n",
    "\n",
    "\n",
    "query=\"NLP Python\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "a) This question deals with the similarity of documents in terms of comparing the words they contain. Describe two approaches to measuring the similarity of individual words. (4 marks)"
   ],
   "metadata": {
    "id": "e7O0BhgAcnqR"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cosine Similarity:  For words represented as vectors in a term-frequency space, the cosine similarity is calculated \n",
    "using the dot product of the vectors. cos = dot(AB) / |A|*|B|\n",
    "\n",
    "Jaccard Similarity:  Jaccard similarity measures the similarity between two sets by comparing the intersection of the\n",
    " sets to their union. intersection(AB) / union(AB)\n",
    " \n",
    " "
   ],
   "metadata": {
    "id": "hROzVWsZeg8w"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "b)\n",
    "i) Write a function that takes a set of tokenized documents as input and returns the same documents after removing stopwords and punctuation, and applying case normalisation to the remaining tokens. So for example, the documents `[[\"This\", \"is\", \"Alice\", \"'s\", \"document\", \".\"], [\"Bob\", \"'s\", \"document\", \".\"]]` would become `[[\"alice\", \"document\"], [\"bob\", \"document\"]]`. Apply this function to the list `documents` to produce a list called `normalised_documents`. (10 marks)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def normalise(documents):\n",
    "    normalised_documents = []\n",
    "    for sentence in documents:\n",
    "        sent = []\n",
    "        for w in sentence:\n",
    "            if w not in stops and w.isalpha():\n",
    "                sent.append(w.lower())\n",
    "        normalised_documents.append(sent)\n",
    "    return normalised_documents\n",
    "\n",
    "normalised_documents = normalise(documents)\n",
    "print(normalised_documents)\n"
   ],
   "metadata": {
    "id": "Mex62utnO52M",
    "ExecuteTime": {
     "end_time": "2024-01-14T18:07:35.568813900Z",
     "start_time": "2024-01-14T18:07:35.564804800Z"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nltk', 'natural', 'language', 'python', 'package', 'use', 'nlp'], ['programming', 'nlp', 'pseudoscientific', 'approach', 'personal', 'development', 'psychotherapy'], ['python', 'genus', 'constricting', 'snakes', 'pythonidae', 'family']]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "ii) What impact will stopword removal and case normalisation have on the similarity of documents? (4 marks)"
   ],
   "metadata": {
    "id": "nKGAf0XhUvRZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It will make the words about main content in the paper more important, and make the document similarity more \n",
    "meaningful and accurate."
   ],
   "metadata": {
    "id": "pPqqwMugVB8K"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "iii) Write a function that takes a list of tokenized documents as input and produces a list of bag-of-words representations. That is, each document should be represented as a FreqDist dictionary mapping tokens to the count of each token in the document. For example, the document `[\"alice\", \"document\"]` would become `FreqDist({'alice': 1, 'document':1})`. Apply this function to the list `normalised_documents` to produce a list called `bags`. (6 marks)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def bag_of_words_representation(normalised_documents):\n",
    "    bags = []\n",
    "    for sentence in normalised_documents:\n",
    "        bag_of_words = FreqDist(sentence)\n",
    "        bags.append(bag_of_words)\n",
    "    return bags\n",
    "\n",
    "bags = bag_of_words_representation(normalised_documents)\n",
    "print(bags)"
   ],
   "metadata": {
    "id": "m5df_L3XlcG1",
    "ExecuteTime": {
     "end_time": "2024-01-14T18:07:38.795887500Z",
     "start_time": "2024-01-14T18:07:38.788366800Z"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FreqDist({'nltk': 1, 'natural': 1, 'language': 1, 'python': 1, 'package': 1, 'use': 1, 'nlp': 1}), FreqDist({'programming': 1, 'nlp': 1, 'pseudoscientific': 1, 'approach': 1, 'personal': 1, 'development': 1, 'psychotherapy': 1}), FreqDist({'python': 1, 'genus': 1, 'constricting': 1, 'snakes': 1, 'pythonidae': 1, 'family': 1})]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "iv) Given the definition below, write a function that takes a list of bag-of-words document representations as input and calculates inverse document frequencies. The output of the function should be dictionary that has tokens as keys and inverse document frequencies as values. \n",
    "\n",
    "$$ idf(t) = \\log_2 \\left( \\frac{N}{df(t)} \\right) $$\n",
    "\n",
    "Here, $df(t)$, is the document frequency of the token, $t$, i.e. the number of documents it occurs in, and $N$ is the total number of documents. For example, if alice occurs in 1 out of 2 documents, then the inverse document frequency dictionary should return the value $1 = \\log_2(2)$ for the key `'alice'`.\n",
    "\n",
    "Apply this function to the list `bags` to produce a dictionary called `idf`.\n",
    "\n",
    "(4 marks)"
   ],
   "metadata": {
    "id": "E2tM7oS-nxzU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def calculate_idf(bags):\n",
    "    N = len(bags)\n",
    "    idf = {}\n",
    "\n",
    "    document_frequencies = {}\n",
    "    for bag in bags:\n",
    "        for token in bag.keys():\n",
    "            document_frequencies[token] = document_frequencies.get(token, 0) + 1\n",
    "\n",
    "    for token, df in document_frequencies.items():\n",
    "        idf[token] = log((N / df), 2)\n",
    "\n",
    "    return idf\n",
    "\n",
    "idf = calculate_idf(bags)\n",
    "print(idf)"
   ],
   "metadata": {
    "id": "EKW-BWQyhL4X",
    "ExecuteTime": {
     "end_time": "2024-01-14T18:07:41.823674300Z",
     "start_time": "2024-01-14T18:07:41.821165800Z"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nltk': 1.5849625007211563, 'natural': 1.5849625007211563, 'language': 1.5849625007211563, 'python': 0.5849625007211562, 'package': 1.5849625007211563, 'use': 1.5849625007211563, 'nlp': 0.5849625007211562, 'programming': 1.5849625007211563, 'pseudoscientific': 1.5849625007211563, 'approach': 1.5849625007211563, 'personal': 1.5849625007211563, 'development': 1.5849625007211563, 'psychotherapy': 1.5849625007211563, 'genus': 1.5849625007211563, 'constricting': 1.5849625007211563, 'snakes': 1.5849625007211563, 'pythonidae': 1.5849625007211563, 'family': 1.5849625007211563}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "v) Write a function that takes a list of bag-of-words document representations and a dictionary of inverse document frequencies and produces a new set of $tfidf$ representations. Apply this function to the list `bags` and the dictionary `idf` to produce a list of document representations called `tfidf_reps`. (4 marks)"
   ],
   "metadata": {
    "id": "tiMtVFZUpy4m"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def calculate_tfidf(bags, idf):\n",
    "    tfidf_reps = []\n",
    "\n",
    "    for bag in bags:\n",
    "        tfidf_rep = {}\n",
    "        total_tokens = sum(bag.values())\n",
    "\n",
    "        for token, tf in bag.items():\n",
    "            tfidf_rep[token] = tf * idf.get(token, 0) / total_tokens\n",
    "\n",
    "        tfidf_reps.append(tfidf_rep)\n",
    "\n",
    "    return tfidf_reps\n",
    "\n",
    "tfidf_reps = calculate_tfidf(bags, idf)\n",
    "\n",
    "for tfidf_rep in tfidf_reps:\n",
    "    print(tfidf_rep)\n"
   ],
   "metadata": {
    "id": "6ZhX6bXlqpdz",
    "ExecuteTime": {
     "end_time": "2024-01-14T18:07:43.805315900Z",
     "start_time": "2024-01-14T18:07:43.803234400Z"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nltk': 0.2264232143887366, 'natural': 0.2264232143887366, 'language': 0.2264232143887366, 'python': 0.08356607153159375, 'package': 0.2264232143887366, 'use': 0.2264232143887366, 'nlp': 0.08356607153159375}\n",
      "{'programming': 0.2264232143887366, 'nlp': 0.08356607153159375, 'pseudoscientific': 0.2264232143887366, 'approach': 0.2264232143887366, 'personal': 0.2264232143887366, 'development': 0.2264232143887366, 'psychotherapy': 0.2264232143887366}\n",
      "{'python': 0.0974937501201927, 'genus': 0.26416041678685936, 'constricting': 0.26416041678685936, 'snakes': 0.26416041678685936, 'pythonidae': 0.26416041678685936, 'family': 0.26416041678685936}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "c)\n",
    "i) Write a function that takes a pair of bag-of-word representations and calculates their dot product. (4 marks)"
   ],
   "metadata": {
    "id": "7KT7us83rXdJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def calculate_dot_product(bag1, bag2):\n",
    "    \n",
    "    common_tokens = set(bag1.keys()) & set(bag2.keys())\n",
    "    dot_product = sum(bag1[token] * bag2[token] for token in common_tokens)\n",
    "    return dot_product\n",
    "\n",
    "dot = calculate_dot_product(bags[0], bags[1])\n",
    "print(dot)"
   ],
   "metadata": {
    "id": "SvkfFyjOztMZ",
    "ExecuteTime": {
     "end_time": "2024-01-14T18:07:46.922671100Z",
     "start_time": "2024-01-14T18:07:46.915150100Z"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "ii) Write a function that takes a pair of vectors and calculates their cosine similarity. (4 marks)"
   ],
   "metadata": {
    "id": "GsKt6dd1z8pj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def calculate_cosine_similarity(vector1, vector2):\n",
    "    dot_product = calculate_dot_product(vector1, vector2)\n",
    "\n",
    "    magnitude1 = sqrt(sum(value**2 for value in vector1.values()))\n",
    "    magnitude2 = sqrt(sum(value**2 for value in vector2.values()))\n",
    "\n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0  # To handle the case where one or both vectors have zero magnitude\n",
    "\n",
    "    cosine_similarity = dot_product / (magnitude1 * magnitude2)\n",
    "    return cosine_similarity\n",
    "\n",
    "cosine_similarity_result = calculate_cosine_similarity(bags[0], bags[1])\n",
    "print(cosine_similarity_result)"
   ],
   "metadata": {
    "id": "6T2bWTzM0F87",
    "ExecuteTime": {
     "end_time": "2024-01-14T18:07:48.962945700Z",
     "start_time": "2024-01-14T18:07:48.961439100Z"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14285714285714285\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "iii) Put the string in `query` through the appropriate pre-processing to produce a tfidf bag-of-words representation, and find the index of the most similar document. Print out the corresponding document from the list `documents`. (10 marks)"
   ],
   "metadata": {
    "id": "TqDB8B_wcPBX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "query_tokens = [w.lower() for w in nltk.word_tokenize(query) if w.lower() not in stops and w.isalpha()]\n",
    "\n",
    "vocabulary = set()\n",
    "for sentence in normalised_documents:\n",
    "    vocabulary.update(sentence)\n",
    "    \n",
    "tf_documents = bags\n",
    "vocabulary.update(query_tokens)\n",
    "\n",
    "document_count = len(normalised_documents)\n",
    "idf_values = {term: log(document_count / sum(term in doc for doc in normalised_documents)) for term in vocabulary}\n",
    "\n",
    "tfidf_documents = []\n",
    "for tf_doc in tf_documents:\n",
    "    tfidf_doc = {term: tf * idf_values[term] for term, tf in tf_doc.items()}\n",
    "    tfidf_documents.append(tfidf_doc)\n",
    "\n",
    "tfidf_query = {term: query_tokens.count(term) * idf_values[term] for term in vocabulary}\n",
    "\n",
    "cosine_similarities = [calculate_cosine_similarity(tfidf_query, tfidf_doc) for tfidf_doc in tfidf_documents]\n",
    "print(cosine_similarities)\n",
    "# vi) Find the index of the most similar document\n",
    "most_similar_index = max(range(len(cosine_similarities)), key=cosine_similarities.__getitem__)\n",
    "\n",
    "print(\"Most similar document:\")\n",
    "print(\" \".join(documents[most_similar_index]))"
   ],
   "metadata": {
    "id": "b9AGQFsJz7zv",
    "ExecuteTime": {
     "end_time": "2024-01-14T18:07:52.228523400Z",
     "start_time": "2024-01-14T18:07:52.220526200Z"
    }
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22731013440410222, 0.1053522533853186, 0.11515227714480396]\n",
      "Most similar document:\n",
      "NLTK , or Natural Language Toolkit, is a Python package that you can use for NLP .\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
